{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miguel1897/01-Tarea-Repaso-ReactJs/blob/master/DeteccionEnfermedades.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFrIjyhpvrkm"
      },
      "source": [
        "Importación de Librerías\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaRVGIuVvz6B",
        "outputId": "a6ae3ebf-5fc6-4dbb-f67d-dd30df4e7e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# librerías necesarias\n",
        "!pip install -q kaggle\n",
        "!pip install torch torchvision\n",
        "\n",
        "# Importa las librerías\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz0cErME9Mbl"
      },
      "source": [
        "Configuración de la API de Kaggle y Descarga del Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn_zV1OK9-RP",
        "outputId": "172a75f2-26f8-48a5-a337-2fa5b909fa6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/kanishk3813/pathogen-dataset\n",
            "License(s): apache-2.0\n",
            "Downloading pathogen-dataset.zip to /content\n",
            " 99% 1.42G/1.43G [00:20<00:00, 71.0MB/s]\n",
            "100% 1.43G/1.43G [00:20<00:00, 73.8MB/s]\n",
            "pathogen  sample_data\n"
          ]
        }
      ],
      "source": [
        "# Configura el archivo de autenticación kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Descarga el dataset desde Kaggle\n",
        "!kaggle datasets download -d kanishk3813/pathogen-dataset --unzip\n",
        "\n",
        "# Verifica la ubicación del dataset descargado\n",
        "!ls /content\n",
        "\n",
        "# Ajusta las rutas para cargar el dataset en PyTorch\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# Define la ruta del directorio según el nombre de la carpeta descomprimida\n",
        "dataset_dir = '/content/pathogen-dataset'  # Cambia esto si el nombre de la carpeta es diferente\n",
        "\n",
        "# Transforma las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZFeNTXT-Cip"
      },
      "source": [
        "Preparación del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gtOrVhZ3-Eau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4f1ad6-c693-496f-dc2d-0b3ee2aaacfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases: ['Bacteria', 'Fungi', 'Healthy', 'Pests', 'Virus']\n",
            "Tamaño del conjunto de entrenamiento: 31997\n",
            "Tamaño del conjunto de validación: 8000\n",
            "Primeras clases de entrenamiento: [37797, 34465, 32332, 6109, 8686]\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# Especifica la ruta del dataset descargado\n",
        "dataset_dir = '/content/pathogen'  # Ruta donde están las carpetas de clases\n",
        "\n",
        "# Define las transformaciones para las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Redimensionar las imágenes a 128x128\n",
        "    transforms.ToTensor()  # Convertir las imágenes a tensores\n",
        "])\n",
        "\n",
        "# Cargar el dataset completo con todas las clases\n",
        "dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n",
        "\n",
        "# Verificar las clases detectadas en el dataset\n",
        "print(\"Clases:\", dataset.classes)\n",
        "\n",
        "# Dividir el dataset en entrenamiento y validación (80% entrenamiento, 20% validación)\n",
        "train_size = int(0.8 * len(dataset))  # 80% para entrenamiento\n",
        "valid_size = len(dataset) - train_size  # El resto para validación\n",
        "\n",
        "# Dividir el dataset en dos\n",
        "train_data, valid_data = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "# Crear los DataLoaders para entrenamiento y validación\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=32)\n",
        "\n",
        "# Verificar la cantidad de muestras en los DataLoaders\n",
        "print(f'Tamaño del conjunto de entrenamiento: {len(train_data)}')\n",
        "print(f'Tamaño del conjunto de validación: {len(valid_data)}')\n",
        "\n",
        "# Verificar algunas clases\n",
        "print(f'Primeras clases de entrenamiento: {train_data.indices[:5]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS_53sFq-IuY"
      },
      "source": [
        "Definición del Modelo CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nArZDdTC-Lfk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PlagaDetectorCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PlagaDetectorCNN, self).__init__()\n",
        "\n",
        "        # Capas convolucionales\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Conv1: 3 canales de entrada, 32 canales de salida\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Conv2: 32 canales de entrada, 64 de salida\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Conv3: 64 canales de entrada, 128 de salida\n",
        "\n",
        "        # Capa de max-pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Tamaño del kernel 2x2\n",
        "\n",
        "        # Capas completamente conectadas\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 512)  # Capa densa (128x16x16 es la dimensión de salida de las convoluciones)\n",
        "        self.fc2 = nn.Linear(512, len(train_data.classes))  # La salida final tiene tantas clases como train_data.classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aplicamos las convoluciones seguidas de ReLU y MaxPooling\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "\n",
        "        # Aplanamos la salida para pasarla a la capa totalmente conectada\n",
        "        x = x.view(-1, 128 * 16 * 16)  # Aplanar: las dimensiones deben coincidir con la capa fc1\n",
        "\n",
        "        # Capas completamente conectadas\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkH_BDo6-NbL"
      },
      "source": [
        "Configuración del Dispositivo (CPU o GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wAIHuj9g-O4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "106cb633-2279-4f4f-a235-bc983126c9dd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-dff51791ae83>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Ahora crear el modelo con el número de clases correcto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlagaDetectorCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ],
      "source": [
        "# Suponiendo que train_data_full es el conjunto de datos completo ImageFolder\n",
        "train_data_full = datasets.ImageFolder(os.path.join(dataset_dir), transform=transform)\n",
        "\n",
        "# Obtener el número de clases\n",
        "num_classes = len(train_data_full.classes)\n",
        "\n",
        "# Definir el modelo usando el número de clases\n",
        "class PlagaDetectorCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(PlagaDetectorCNN, self).__init__()\n",
        "\n",
        "        # Capas convolucionales\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)  # Usar num_classes aquí\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 16 * 16)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Ahora crear el modelo con el número de clases correcto\n",
        "model = PlagaDetectorCNN(num_classes).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSGxi5a-SJC"
      },
      "source": [
        "Definición de Funciones de Entrenamiento y Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmAEPV6D-UGh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define el criterio de pérdida y el optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Función para entrenar el modelo\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Mover a GPU/CPU\n",
        "            optimizer.zero_grad()  # Limpiar gradientes previos\n",
        "            outputs = model(images)  # Obtener las predicciones del modelo\n",
        "            loss = criterion(outputs, labels)  # Calcular la pérdida\n",
        "            loss.backward()  # Calcular el gradiente\n",
        "            optimizer.step()  # Actualizar los parámetros\n",
        "            running_loss += loss.item()  # Acumular la pérdida de la época\n",
        "        # Promediar la pérdida de la época\n",
        "        print(f\"Epoch {epoch+1}, Pérdida promedio: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Función para evaluar el modelo en el conjunto de validación\n",
        "def evaluate_model(model, valid_loader):\n",
        "    model.eval()  # Modo evaluación (desactiva dropout, etc.)\n",
        "    total, correct = 0, 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():  # No calcular gradientes durante la validación\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)  # Obtener las predicciones del modelo\n",
        "            loss = criterion(outputs, labels)  # Calcular la pérdida de validación\n",
        "            running_loss += loss.item()  # Acumular la pérdida de validación\n",
        "            _, predicted = torch.max(outputs, 1)  # Obtener las predicciones (la clase con mayor probabilidad)\n",
        "            total += labels.size(0)  # Total de muestras\n",
        "            correct += (predicted == labels).sum().item()  # Comparar con las etiquetas reales\n",
        "\n",
        "    # Imprimir métricas\n",
        "    print(f\"Pérdida en validación: {running_loss/len(valid_loader)}\")\n",
        "    print(f\"Precisión en validación: {100 * correct / total}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLdhlxNa-YHj"
      },
      "source": [
        "Entrenamiento del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ieej0Tk__ACo",
        "outputId": "7abd3857-6b5c-4eba-e394-f04d82ccf5b0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e057ac09b8cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "evaluate_model(model, valid_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfiGjp18_B4d"
      },
      "source": [
        "Uso del Modelo para Predicciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU4Kwrr-_DTm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configuración del dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Utilizando un modelo preentrenado para mejorar el rendimiento (ResNet18 en este caso)\n",
        "class PlagaDetectorResNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(PlagaDetectorResNet, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Definición de transformaciones de datos con data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Cargar los datos de entrenamiento y validación\n",
        "dataset_dir = '/content/pathogen'  # Asegúrate de que esta ruta sea correcta\n",
        "train_data = datasets.ImageFolder(dataset_dir, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(train_data, batch_size=32)\n",
        "\n",
        "# Inicializar el modelo, pérdida y optimizador\n",
        "model = PlagaDetectorResNet(num_classes=len(train_data.classes)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# Función para entrenar el modelo\n",
        "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=15):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        scheduler.step()  # Actualiza el learning rate\n",
        "        print(f\"Epoch {epoch+1}, Pérdida promedio: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Función para evaluar el modelo en el conjunto de validación usando F1-score\n",
        "def evaluate_model(model, valid_loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    running_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calcula precisión y F1-score\n",
        "    accuracy = 100 * correct / total\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "    print(f\"Pérdida en validación: {running_loss/len(valid_loader):.4f}\")\n",
        "    print(f\"Precisión en validación: {accuracy:.2f}%\")\n",
        "    print(f\"F1-Score en validación: {f1:.4f}\")\n",
        "\n",
        "# Entrena y evalúa el modelo\n",
        "train_model(model, train_loader, criterion, optimizer, scheduler, epochs=15)\n",
        "evaluate_model(model, valid_loader)\n",
        "\n",
        "# Función de predicción para una imagen individual\n",
        "def predict_image(image_path, model):\n",
        "    model.eval()\n",
        "    transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "    output = model(image)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return train_data.classes[predicted.item()]\n",
        "\n",
        "# Prueba con una imagen de ejemplo\n",
        "image_path = '/content/pathogen/Healthy/image.jpg'  # Cambia según tu imagen de prueba\n",
        "prediccion = predict_image(image_path, model)\n",
        "print(f\"Plaga detectada: {prediccion}\")\n",
        "plt.imshow(Image.open(image_path))\n",
        "plt.title(f\"Predicción: {prediccion}\")\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQmTRhWQlnNcm4dna6gdvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}